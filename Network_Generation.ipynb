{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d41cf044",
   "metadata": {},
   "source": [
    "Network Generation + Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c612979",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "368439\n",
      "-----------------------------\n",
      "21091280\n",
      "-----------------------------\n",
      "Total Sentiment 0.01650823248140731\n",
      "Mask Sentiment 0.08409740941137892\n",
      "Doctor Sentiment -0.05267082399103139\n",
      "Nurse Sentiment 0.10144083541147132\n",
      "Vaccine Sentiment 0.09008215927901417\n",
      "Vaccinate Sentiment -0.157518125\n",
      "Lockdown Sentiment 0.03800913097894107\n",
      "Restriction Sentiment 0.16075354424357755\n",
      "School Sentiment 0.10206796187683284\n",
      "Government Sentiment -0.055149519343493554\n",
      "China Sentiment -0.1419472584856397\n",
      "UK Sentiment -0.1420010101010101\n",
      "Canada Sentiment 0.18068385416666666\n",
      "Italy Sentiment 0.056527777777777774\n",
      "India Sentiment 0.13400011560693642\n",
      "Freedom Sentiment 0.2532687\n"
     ]
    }
   ],
   "source": [
    "# The needed NLTK imports\n",
    "import nltk # needed for download statements below (if your machine requires them)\n",
    "from nltk.corpus import stopwords # The list of stopwords \n",
    "from nltk.tokenize import word_tokenize # Word tokenization functionality (string -> list of seperate words)\n",
    "from nltk.tokenize import RegexpTokenizer # allows tokenization with regex expressions\n",
    "from nltk.stem import WordNetLemmatizer # Allows for lemmatization \n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from statistics import mean \n",
    "\n",
    "# Below are 2 downloads you might need to run once.\n",
    "# There may be others (I didnt fully track them), at the bottom the error message will tell you which ones to download\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4') \n",
    "# nltk.download('vader_lexicon')\n",
    "    \n",
    "import re # for regex functions\n",
    "\n",
    "import networkx as nx # networkx\n",
    "import jsonlines as jl # for working with jsonl type files \n",
    "\n",
    "\n",
    "# Helpers with the text processing, seperated for more readability in the function\n",
    "def deEmojify(inputString):\n",
    "    return inputString.encode('ascii', 'ignore').decode('ascii')\n",
    "tokeniser = RegexpTokenizer(r'\\w+')\n",
    "stop = set(stopwords.words('english'))\n",
    "# Adding 'covid' to list of deleted words\n",
    "stop.add('covid')\n",
    "stop.add('coronavirus') \n",
    "stop.add('corona') \n",
    "stop.add('virus') \n",
    "stop.add('amp') \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# setting up sentiment analysis\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "all_sentiment = []\n",
    "mask_sentiment= []\n",
    "doctor_sentiment= []\n",
    "nurse_sentiment= []\n",
    "vaccine_sentiment= []\n",
    "vaccinate_sentiment= []\n",
    "lockdown_sentiment= []\n",
    "restriction_sentiment= []\n",
    "school_sentiment= []\n",
    "government_sentiment= []\n",
    "china_sentiment= []\n",
    "\n",
    "uk_sentiment= []\n",
    "canada_sentiment= []\n",
    "italy_sentiment= []\n",
    "india_sentiment= []\n",
    "freedom_sentiment= []\n",
    "\n",
    "\n",
    "\n",
    "# Pre-processing the tweet\n",
    "def text_processing(tweet_text):\n",
    "\n",
    "    tweet_text = tweet_text.lower() # lowercase\n",
    "\n",
    "    tweet_text = tweet_text.replace('new zealand', 'newzealand') # new zealand case\n",
    "\n",
    "    tweet_text = re.sub(\"(\\S+://\\S+)\", \"\", tweet_text) # URLs\n",
    "\n",
    "    tweet_text = re.sub(\"(\\d+)\", \"\", tweet_text) # Numbers\n",
    "\n",
    "    tweet_text = re.sub(\"[.,!?:;-=-'...\\\"@#_]\", \" \", tweet_text) # symbols + punctuation\n",
    "\n",
    "    tweet_text = deEmojify(tweet_text) # emojis\n",
    "\n",
    "    tweet_text = tokeniser.tokenize(tweet_text) # Tokenization\n",
    "\n",
    "    tweet_text = [word for word in tweet_text if word not in stop] # Stop words\n",
    "\n",
    "    for i in range (0, len(tweet_text)): # Verb lemmatization\n",
    "        tweet_text[i] = lemmatizer.lemmatize(tweet_text[i], pos='v')\n",
    "        \n",
    "    tweet_text = list(set(tweet_text)) # Removing duplicate words\n",
    "    \n",
    "    return tweet_text # returned processed tweet as a list\n",
    "\n",
    "   \n",
    "# Initializing the networkx graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Adding nodes and edges from the tweets to the nx graph\n",
    "# Set hydrated jsonl tweet file below\n",
    "with jl.open('15.may_29_2020_world_death_toll_1000000.jsonl') as json_Lines:\n",
    "    for json_obj in json_Lines:\n",
    "        for i in range(0,len(json_obj[\"data\"])):\n",
    "            \n",
    "            tweet = json_obj[\"data\"][i][\"text\"]            \n",
    "\n",
    "            processed_tweet = text_processing(tweet)             \n",
    "            \n",
    "            # SENTIMENT ANALYSIS\n",
    "            # getting the compound polarity score of the tweet\n",
    "            sentiment = sia.polarity_scores(tweet)['compound']\n",
    "            \n",
    "            # Removing null sentiments\n",
    "            if sentiment != 0:\n",
    "                all_sentiment.append(sentiment)\n",
    "\n",
    "            \n",
    "            # Sentiment of tweets with particular keywords in them\n",
    "            \n",
    "            if \"mask\" in tweet:              \n",
    "                if sentiment != 0:\n",
    "                    mask_sentiment.append(sentiment)\n",
    "            \n",
    "            if \"doctor\" in tweet:              \n",
    "                if sentiment != 0:\n",
    "                    doctor_sentiment.append(sentiment)\n",
    "            \n",
    "            if \"nurse\" in tweet:              \n",
    "                if sentiment != 0:\n",
    "                    nurse_sentiment.append(sentiment)\n",
    "                        \n",
    "\n",
    "            if \"vaccine\" in tweet:              \n",
    "                if sentiment != 0:\n",
    "                    vaccine_sentiment.append(sentiment)\n",
    "            \n",
    "            if \"vaccinate\" in tweet:              \n",
    "                if sentiment != 0:\n",
    "                    vaccinate_sentiment.append(sentiment)\n",
    "            \n",
    "            if \"lockdown\" in tweet:              \n",
    "                if sentiment != 0:\n",
    "                    lockdown_sentiment.append(sentiment)\n",
    "            \n",
    "            if \"restriction\" in tweet:              \n",
    "                if sentiment != 0:\n",
    "                    restriction_sentiment.append(sentiment)\n",
    "            \n",
    "            if \"school\" in tweet:              \n",
    "                if sentiment != 0:\n",
    "                    school_sentiment.append(sentiment)\n",
    "            \n",
    "            if \"government\" in tweet:              \n",
    "                if sentiment != 0:\n",
    "                    government_sentiment.append(sentiment)\n",
    "            \n",
    "            if \"china\" in tweet:              \n",
    "                if sentiment != 0:\n",
    "                    china_sentiment.append(sentiment)\n",
    "                                    \n",
    "            if \" uk \" in tweet:              \n",
    "                if sentiment != 0:\n",
    "                    uk_sentiment.append(sentiment)\n",
    "                    \n",
    "            if \"canada\" in tweet:              \n",
    "                if sentiment != 0:\n",
    "                    canada_sentiment.append(sentiment)\n",
    "                    \n",
    "            if \"italy\" in tweet:              \n",
    "                if sentiment != 0:\n",
    "                    italy_sentiment.append(sentiment)\n",
    "                    \n",
    "            if \"india\" in tweet:              \n",
    "                if sentiment != 0:\n",
    "                    india_sentiment.append(sentiment)\n",
    "                    \n",
    "            if \"freedom\" in tweet:              \n",
    "                if sentiment != 0:\n",
    "                    freedom_sentiment.append(sentiment)\n",
    "                    \n",
    "            \n",
    "            # add all unique words as nodes from the tweets\n",
    "            # add_nodes_from will not add duplicates\n",
    "            G.add_nodes_from(processed_tweet) \n",
    "            \n",
    "                       \n",
    "            # now need to add edges between all the word nodes we have just added\n",
    "            # for each word of the tweet\n",
    "            for j in range(0,len(processed_tweet)):\n",
    "                \n",
    "                #For each word after that word\n",
    "                for k in range(j + 1,len(processed_tweet)): \n",
    "                    \n",
    "                    # if the edge exists already for the 2 words, increase weight by one\n",
    "                    if G.has_edge(processed_tweet[j], processed_tweet[k]): \n",
    "                        G.edges[processed_tweet[j], processed_tweet[k]]['weight'] += 1\n",
    "                    \n",
    "                    # else add a new edge between words, of weight 1\n",
    "                    else:                                                    \n",
    "                        G.add_edge(processed_tweet[j], processed_tweet[k], weight=1)\n",
    "                        \n",
    "                        \n",
    "\n",
    "# printing the number of words and edges (Ensuring nothing went catastrophically wrong)                          \n",
    "\n",
    "print(\"-----------------------------\")\n",
    "print(len(G.nodes))\n",
    "print(\"-----------------------------\")\n",
    "print(len(G.edges))\n",
    "print(\"-----------------------------\")\n",
    "\n",
    "print(\"Total Sentiment\", mean(all_sentiment))\n",
    "\n",
    "print(\"Mask Sentiment\", mean(mask_sentiment))\n",
    "print(\"Doctor Sentiment\", mean(doctor_sentiment))\n",
    "print(\"Nurse Sentiment\", mean(nurse_sentiment))\n",
    "print(\"Vaccine Sentiment\", mean(vaccine_sentiment))\n",
    "print(\"Vaccinate Sentiment\", mean(vaccinate_sentiment))\n",
    "print(\"Lockdown Sentiment\", mean(lockdown_sentiment))\n",
    "print(\"Restriction Sentiment\", mean(restriction_sentiment))\n",
    "print(\"School Sentiment\", mean(school_sentiment))\n",
    "print(\"Government Sentiment\", mean(government_sentiment))\n",
    "\n",
    "print(\"China Sentiment\", mean(china_sentiment))\n",
    "print(\"UK Sentiment\", mean(uk_sentiment))\n",
    "print(\"Canada Sentiment\", mean(canada_sentiment))\n",
    "print(\"Italy Sentiment\", mean(italy_sentiment))\n",
    "print(\"India Sentiment\", mean(india_sentiment))\n",
    "print(\"Freedom Sentiment\", mean(freedom_sentiment))\n",
    "\n",
    "\n",
    "#UNCOMMENT OUT BELOW IF YOU WANT TO GENERATE A GEXF FILE FOR GEPHI VISUALIZATION\n",
    "\n",
    "# making a gexf file (for Gephi) of the networkx network\n",
    "# nx.write_gexf(G, \"large_network.gexf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d683ab",
   "metadata": {},
   "source": [
    "Degree Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3e212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx # networkx\n",
    "import numpy as np # numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import logging\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "import networkx.algorithms.community as nx_comm\n",
    "\n",
    "# reading from gexf file\n",
    "G = nx.read_gexf(\"large_network.gexf\")\n",
    "\n",
    "# Function to plot a degree distribution graph (P(k) vs k graph)\n",
    "# Code taken from CPSC 572 tutorial \n",
    "def plot_degree_dist(G):\n",
    "    \n",
    "    degrees = [G.degree(n) for n in G.nodes()]\n",
    "    kmin = min(degrees)\n",
    "    kmax = max(degrees)\n",
    "    \n",
    "    if kmin>0:\n",
    "        bin_edges = np.logspace(np.log10(kmin), np.log10(kmax)+1, num=20)\n",
    "    else:\n",
    "        bin_edges = np.logspace(0, np.log10(kmax)+1, num=30)\n",
    "    density, _ = np.histogram(degrees, bins=bin_edges, density=True)\n",
    "\n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "    log_be = np.log10(bin_edges)    \n",
    "    x = 10**((log_be[1:] + log_be[:-1])/2) \n",
    "    \n",
    "    plt.loglog(x, density, marker='o', linestyle='none')    \n",
    "     \n",
    "    plt.xlabel(r\"degree $k$\", fontsize=16)\n",
    "    plt.ylabel(r\"$P(k)$\", fontsize=16)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# SHOW DEGREE DISTRIBUTION\n",
    "plot_degree_dist(G)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfadbda4",
   "metadata": {},
   "source": [
    "Top X eigencentral nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b6cd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx # networkx\n",
    "import numpy as np # numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import logging\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "import networkx.algorithms.community as nx_comm\n",
    "\n",
    "# reading from gexf file\n",
    "G = nx.read_gexf(\"large_network.gexf\")\n",
    "\n",
    "# # A list of the top eigan nodes and their eigan values\n",
    "eigans =  sorted(nx.eigenvector_centrality(G, weight='weight').items(), key=lambda x:x[1])\n",
    "eigans.reverse()\n",
    "\n",
    "# range (X)\n",
    "for i in range(200):\n",
    "    print(i+1, \":\", eigans[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c937c8b",
   "metadata": {},
   "source": [
    "Top X other words used in the same tweet with a designated word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86ad83c1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : ('mask', 'wear', {'id': '160330', 'weight': 49.0})\n",
      "2 : ('face', 'mask', {'id': '59167', 'weight': 49.0})\n",
      "3 : ('mask', 'distance', {'id': '160390', 'weight': 19.0})\n",
      "4 : ('mask', 'use', {'id': '160292', 'weight': 17.0})\n",
      "5 : ('people', 'mask', {'id': '152556', 'weight': 17.0})\n",
      "\n",
      "\n",
      "\n",
      "1 : ('test', 'doctor', {'id': '119897', 'weight': 11.0})\n",
      "2 : ('patients', 'doctor', {'id': '8622', 'weight': 10.0})\n",
      "3 : ('get', 'doctor', {'id': '11453', 'weight': 9.0})\n",
      "4 : ('doctor', 'nurse', {'id': '351173', 'weight': 7.0})\n",
      "5 : ('die', 'doctor', {'id': '343747', 'weight': 7.0})\n",
      "\n",
      "\n",
      "\n",
      "1 : ('home', 'nurse', {'id': '804', 'weight': 17.0})\n",
      "2 : ('doctor', 'nurse', {'id': '351173', 'weight': 7.0})\n",
      "3 : ('state', 'nurse', {'id': '352083', 'weight': 6.0})\n",
      "4 : ('health', 'nurse', {'id': '189815', 'weight': 6.0})\n",
      "5 : ('pandemic', 'nurse', {'id': '45922', 'weight': 5.0})\n",
      "\n",
      "\n",
      "\n",
      "1 : ('vaccine', 'astrazeneca', {'id': '303421', 'weight': 11.0})\n",
      "2 : ('china', 'vaccine', {'id': '182495', 'weight': 10.0})\n",
      "3 : ('say', 'vaccine', {'id': '133556', 'weight': 9.0})\n",
      "4 : ('vaccine', 'oxford', {'id': '303469', 'weight': 8.0})\n",
      "5 : ('good', 'vaccine', {'id': '284718', 'weight': 8.0})\n",
      "\n",
      "\n",
      "\n",
      "1 : ('vaccinate', 'johnjharwood', {'id': '557146', 'weight': 1.0})\n",
      "2 : ('wuflu', 'vaccinate', {'id': '557144', 'weight': 1.0})\n",
      "3 : ('ca', 'vaccinate', {'id': '545515', 'weight': 1.0})\n",
      "4 : ('john', 'vaccinate', {'id': '523597', 'weight': 1.0})\n",
      "5 : ('miss', 'vaccinate', {'id': '482903', 'weight': 1.0})\n",
      "\n",
      "\n",
      "\n",
      "1 : ('india', 'lockdown', {'id': '129461', 'weight': 39.0})\n",
      "2 : ('lockdown', 'due', {'id': '141572', 'weight': 32.0})\n",
      "3 : ('take', 'lockdown', {'id': '79619', 'weight': 32.0})\n",
      "4 : ('face', 'lockdown', {'id': '59170', 'weight': 31.0})\n",
      "5 : ('us', 'lockdown', {'id': '29346', 'weight': 31.0})\n",
      "\n",
      "\n",
      "\n",
      "1 : ('britain', 'restriction', {'id': '449539', 'weight': 2.0})\n",
      "2 : ('isolate', 'restriction', {'id': '415764', 'weight': 2.0})\n",
      "3 : ('arrive', 'restriction', {'id': '415107', 'weight': 2.0})\n",
      "4 : ('self', 'restriction', {'id': '414770', 'weight': 2.0})\n",
      "5 : ('monday', 'restriction', {'id': '325222', 'weight': 2.0})\n",
      "\n",
      "\n",
      "\n",
      "1 : ('school', 'children', {'id': '316271', 'weight': 11.0})\n",
      "2 : ('schoolreopening', 'school', {'id': '125916', 'weight': 11.0})\n",
      "3 : ('close', 'school', {'id': '126487', 'weight': 10.0})\n",
      "4 : ('reopen', 'school', {'id': '84974', 'weight': 9.0})\n",
      "5 : ('go', 'school', {'id': '49350', 'weight': 9.0})\n",
      "\n",
      "\n",
      "\n",
      "1 : ('government', 'people', {'id': '2958', 'weight': 27.0})\n",
      "2 : ('government', 'pandemic', {'id': '3090', 'weight': 24.0})\n",
      "3 : ('government', 'uk', {'id': '2879', 'weight': 21.0})\n",
      "4 : ('government', 'support', {'id': '3087', 'weight': 18.0})\n",
      "5 : ('government', 'make', {'id': '2857', 'weight': 18.0})\n",
      "\n",
      "\n",
      "\n",
      "1 : ('us', 'china', {'id': '29099', 'weight': 16.0})\n",
      "2 : ('say', 'china', {'id': '133288', 'weight': 14.0})\n",
      "3 : ('china', 'state', {'id': '182547', 'weight': 13.0})\n",
      "4 : ('china', 'senator', {'id': '182598', 'weight': 11.0})\n",
      "5 : ('china', 'vaccine', {'id': '182495', 'weight': 10.0})\n",
      "\n",
      "\n",
      "\n",
      "1 : ('quarantine', 'uk', {'id': '128589', 'weight': 40.0})\n",
      "2 : ('say', 'uk', {'id': '133275', 'weight': 36.0})\n",
      "3 : ('news', 'uk', {'id': '69390', 'weight': 30.0})\n",
      "4 : ('us', 'uk', {'id': '29275', 'weight': 28.0})\n",
      "5 : ('rule', 'uk', {'id': '44392', 'weight': 27.0})\n",
      "\n",
      "\n",
      "\n",
      "1 : ('usa', 'canada', {'id': '371647', 'weight': 5.0})\n",
      "2 : ('nonprofits', 'canada', {'id': '506615', 'weight': 3.0})\n",
      "3 : ('address', 'canada', {'id': '487444', 'weight': 3.0})\n",
      "4 : ('international', 'canada', {'id': '480802', 'weight': 3.0})\n",
      "5 : ('tourism', 'canada', {'id': '461697', 'weight': 3.0})\n",
      "\n",
      "\n",
      "\n",
      "1 : ('italy', 'france', {'id': '374742', 'weight': 4.0})\n",
      "2 : ('first', 'italy', {'id': '287953', 'weight': 3.0})\n",
      "3 : ('take', 'italy', {'id': '79356', 'weight': 3.0})\n",
      "4 : ('like', 'italy', {'id': '33574', 'weight': 3.0})\n",
      "5 : ('italy', 'spain', {'id': '374745', 'weight': 2.0})\n",
      "\n",
      "\n",
      "\n",
      "1 : ('case', 'india', {'id': '19750', 'weight': 42.0})\n",
      "2 : ('india', 'lockdown', {'id': '129461', 'weight': 39.0})\n",
      "3 : ('number', 'india', {'id': '86489', 'weight': 30.0})\n",
      "4 : ('take', 'india', {'id': '79387', 'weight': 29.0})\n",
      "5 : ('india', 'outbreak', {'id': '129650', 'weight': 25.0})\n",
      "\n",
      "\n",
      "\n",
      "1 : ('say', 'freedom', {'id': '133615', 'weight': 3.0})\n",
      "2 : ('us', 'freedom', {'id': '30785', 'weight': 3.0})\n",
      "3 : ('else', 'freedom', {'id': '416711', 'weight': 2.0})\n",
      "4 : ('press', 'freedom', {'id': '327547', 'weight': 2.0})\n",
      "5 : ('authorities', 'freedom', {'id': '294840', 'weight': 2.0})\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx # networkx\n",
    "import numpy as np # numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import logging\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "import networkx.algorithms.community as nx_comm\n",
    "\n",
    "# reading from gexf file\n",
    "G = nx.read_gexf(\"small_network.gexf\")\n",
    "\n",
    "edges = sorted(G.edges(data=True), key=lambda t: t[2].get('weight', 1))\n",
    "edges.reverse()\n",
    "\n",
    "# designated word goes here\n",
    "j = 0\n",
    "X = 5 #number of other top words to find\n",
    "for i in range(len(edges)):\n",
    "    if j == X:\n",
    "        break\n",
    "    # tupal so you have to check both sides of the node\n",
    "    if edges[i][0] == 'mask' or edges[i][1] == 'mask':\n",
    "        j += 1\n",
    "        print(j, \":\", edges[i])\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "j = 0\n",
    "X = 5 #number of other top words to find\n",
    "for i in range(len(edges)):\n",
    "    if j == X:\n",
    "        break\n",
    "    # tupal so you have to check both sides of the node\n",
    "    if edges[i][0] == 'doctor' or edges[i][1] == 'doctor':\n",
    "        j += 1\n",
    "        print(j, \":\", edges[i])\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "j = 0\n",
    "X = 5 #number of other top words to find\n",
    "for i in range(len(edges)):\n",
    "    if j == X:\n",
    "        break\n",
    "    # tupal so you have to check both sides of the node\n",
    "    if edges[i][0] == 'nurse' or edges[i][1] == 'nurse':\n",
    "        j += 1\n",
    "        print(j, \":\", edges[i])\n",
    "        \n",
    "print(\"\\n\\n\")\n",
    "j = 0\n",
    "X = 5 #number of other top words to find\n",
    "for i in range(len(edges)):\n",
    "    if j == X:\n",
    "        break\n",
    "    # tupal so you have to check both sides of the node\n",
    "    if edges[i][0] == 'vaccine' or edges[i][1] == 'vaccine':\n",
    "        j += 1\n",
    "        print(j, \":\", edges[i])\n",
    " \n",
    "print(\"\\n\\n\")\n",
    "j = 0\n",
    "X = 5 #number of other top words to find\n",
    "for i in range(len(edges)):\n",
    "    if j == X:\n",
    "        break\n",
    "    # tupal so you have to check both sides of the node\n",
    "    if edges[i][0] == 'vaccinate' or edges[i][1] == 'vaccinate':\n",
    "        j += 1\n",
    "        print(j, \":\", edges[i])\n",
    "      \n",
    "print(\"\\n\\n\")    \n",
    "j = 0\n",
    "X = 5 #number of other top words to find\n",
    "for i in range(len(edges)):\n",
    "    if j == X:\n",
    "        break\n",
    "    # tupal so you have to check both sides of the node\n",
    "    if edges[i][0] == 'lockdown' or edges[i][1] == 'lockdown':\n",
    "        j += 1\n",
    "        print(j, \":\", edges[i])\n",
    "      \n",
    "print(\"\\n\\n\")    \n",
    "j = 0\n",
    "X = 5 #number of other top words to find\n",
    "for i in range(len(edges)):\n",
    "    if j == X:\n",
    "        break\n",
    "    # tupal so you have to check both sides of the node\n",
    "    if edges[i][0] == 'restriction' or edges[i][1] == 'restriction':\n",
    "        j += 1\n",
    "        print(j, \":\", edges[i])\n",
    "   \n",
    "print(\"\\n\\n\")\n",
    "j = 0\n",
    "X = 5 #number of other top words to find\n",
    "for i in range(len(edges)):\n",
    "    if j == X:\n",
    "        break\n",
    "    # tupal so you have to check both sides of the node\n",
    "    if edges[i][0] == 'school' or edges[i][1] == 'school':\n",
    "        j += 1\n",
    "        print(j, \":\", edges[i])\n",
    "    \n",
    "print(\"\\n\\n\")    \n",
    "j = 0\n",
    "X = 5 #number of other top words to find\n",
    "for i in range(len(edges)):\n",
    "    if j == X:\n",
    "        break\n",
    "    # tupal so you have to check both sides of the node\n",
    "    if edges[i][0] == 'government' or edges[i][1] == 'government':\n",
    "        j += 1\n",
    "        print(j, \":\", edges[i])\n",
    "     \n",
    "print(\"\\n\\n\")    \n",
    "j = 0\n",
    "X = 5 #number of other top words to find\n",
    "for i in range(len(edges)):\n",
    "    if j == X:\n",
    "        break\n",
    "    # tupal so you have to check both sides of the node\n",
    "    if edges[i][0] == 'china' or edges[i][1] == 'china':\n",
    "        j += 1\n",
    "        print(j, \":\", edges[i])\n",
    "    \n",
    "print(\"\\n\\n\")    \n",
    "j = 0\n",
    "X = 5 #number of other top words to find\n",
    "for i in range(len(edges)):\n",
    "    if j == X:\n",
    "        break\n",
    "    # tupal so you have to check both sides of the node\n",
    "    if edges[i][0] == 'uk' or edges[i][1] == 'uk':\n",
    "        j += 1\n",
    "        print(j, \":\", edges[i])\n",
    "    \n",
    "print(\"\\n\\n\")    \n",
    "j = 0\n",
    "X = 5 #number of other top words to find\n",
    "for i in range(len(edges)):\n",
    "    if j == X:\n",
    "        break\n",
    "    # tupal so you have to check both sides of the node\n",
    "    if edges[i][0] == 'canada' or edges[i][1] == 'canada':\n",
    "        j += 1\n",
    "        print(j, \":\", edges[i])\n",
    "     \n",
    "print(\"\\n\\n\")    \n",
    "j = 0\n",
    "X = 5 #number of other top words to find\n",
    "for i in range(len(edges)):\n",
    "    if j == X:\n",
    "        break\n",
    "    # tupal so you have to check both sides of the node\n",
    "    if edges[i][0] == 'italy' or edges[i][1] == 'italy':\n",
    "        j += 1\n",
    "        print(j, \":\", edges[i])\n",
    "    \n",
    "print(\"\\n\\n\")    \n",
    "j = 0\n",
    "X = 5 #number of other top words to find\n",
    "for i in range(len(edges)):\n",
    "    if j == X:\n",
    "        break\n",
    "    # tupal so you have to check both sides of the node\n",
    "    if edges[i][0] == 'india' or edges[i][1] == 'india':\n",
    "        j += 1\n",
    "        print(j, \":\", edges[i])\n",
    "\n",
    "print(\"\\n\\n\")        \n",
    "j = 0\n",
    "X = 5 #number of other top words to find\n",
    "for i in range(len(edges)):\n",
    "    if j == X:\n",
    "        break\n",
    "    # tupal so you have to check both sides of the node\n",
    "    if edges[i][0] == 'freedom' or edges[i][1] == 'freedom':\n",
    "        j += 1\n",
    "        print(j, \":\", edges[i])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5c6ffb",
   "metadata": {},
   "source": [
    "Position of pandemic keywords in eigan nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fee0bb9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 : ('lockdown', 0.11872186686201039)\n",
      "19 : ('uk', 0.0979377322016567)\n",
      "26 : ('india', 0.08337886987352307)\n",
      "34 : ('government', 0.07696015528150099)\n",
      "96 : ('mask', 0.043740849053814444)\n",
      "155 : ('china', 0.031403762359685033)\n",
      "199 : ('school', 0.026766348004618974)\n",
      "208 : ('doctor', 0.025981965628130117)\n",
      "293 : ('vaccine', 0.020456203020902137)\n",
      "446 : ('nurse', 0.014640031604647217)\n",
      "1385 : ('freedom', 0.004143464242908818)\n",
      "1422 : ('canada', 0.003993890308765853)\n",
      "1453 : ('italy', 0.0038647767824698596)\n",
      "2672 : ('restriction', 0.0017486540666958495)\n",
      "9679 : ('vaccinate', 0.0003870582477794255)\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx # networkx\n",
    "import numpy as np # numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import logging\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "import networkx.algorithms.community as nx_comm\n",
    "\n",
    "# reading from gexf file\n",
    "G = nx.read_gexf(\"small_network.gexf\")\n",
    "\n",
    "# # A list of the top eigan nodes and their eigan values\n",
    "eigans =  sorted(nx.eigenvector_centrality(G, weight='weight').items(), key=lambda x:x[1])\n",
    "eigans.reverse()\n",
    "\n",
    "# range (X)\n",
    "for i in range(len(eigans)):\n",
    "\n",
    "    if eigans[i][0] == 'mask':\n",
    "        print(i+1, \":\", eigans[i])\n",
    "\n",
    "    if eigans[i][0] == 'doctor':\n",
    "        print(i+1, \":\", eigans[i])\n",
    "\n",
    "    if eigans[i][0] == 'nurse':\n",
    "        print(i+1, \":\", eigans[i])\n",
    "\n",
    "    if eigans[i][0] == 'vaccine':\n",
    "        print(i+1, \":\", eigans[i])\n",
    "\n",
    "    if eigans[i][0] == 'vaccinate':\n",
    "        print(i+1, \":\", eigans[i])\n",
    "\n",
    "    if eigans[i][0] == 'lockdown':\n",
    "        print(i+1, \":\", eigans[i])\n",
    "\n",
    "    if eigans[i][0] == 'restriction':\n",
    "        print(i+1, \":\", eigans[i])\n",
    "\n",
    "    if eigans[i][0] == 'school':\n",
    "        print(i+1, \":\", eigans[i])\n",
    "\n",
    "    if eigans[i][0] == 'government':\n",
    "        print(i+1, \":\", eigans[i])\n",
    "\n",
    "    if eigans[i][0] == 'china':\n",
    "        print(i+1, \":\", eigans[i])\n",
    "\n",
    "    if eigans[i][0] == 'uk':\n",
    "        print(i+1, \":\", eigans[i])\n",
    "\n",
    "    if eigans[i][0] == 'canada':\n",
    "        print(i+1, \":\", eigans[i])\n",
    "\n",
    "    if eigans[i][0] == 'italy':\n",
    "        print(i+1, \":\", eigans[i])\n",
    "\n",
    "    if eigans[i][0] == 'india':\n",
    "        print(i+1, \":\", eigans[i])\n",
    "\n",
    "    if eigans[i][0] == 'freedom':\n",
    "        print(i+1, \":\", eigans[i])\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
